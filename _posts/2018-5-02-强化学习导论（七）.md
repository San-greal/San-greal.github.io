---
layout:     post
title:      强化学习导论（七）
subtitle:   
date:       2018-5-2
author:     度朝腾
header-img: img/post-bg-universe.jpg
catalog: true
tags:
    - python
---

## Part I: 表格解决方法

​	在本书的这一部分中，我们以最简单的形式描述了几乎所有强化学习算法的核心思想：算法中的状态和动作空间足够小，可以将近似值函数表示为数组或表格。 在这种情况下，这些方法通常可以找到确切的解决方案，也就是说，他们经常可以找到最佳的价值函数和最优策略。这与本书下一部分描述的近似方法形成对比，后者只找到近似解， 但是作为回报可以有效应用于更大的问题。

​	本书的这一部分的第一章描述了强化学习问题的特殊情况下的解决方法，其中只有一个状态，称为土匪问题。 第二章描述了我们在整个剩余书 - 有限马尔可夫决策过程中处理的一般问题表达式，以及其主要思想，包括Bellman方程和价值函数。

​	接下来的三章描述了解决有限马尔可夫决策问题的三种基本方法：动态规划，蒙特卡洛方法和时间差异学习。每类方法都有其优点和缺点。 动态规划在数学上很完善，但需要完整和准确的环境模型。 蒙特卡罗方法不需要模型，在概念上很简单，但不适合逐步增量计算。 最后，时间差分方法不需要模型，而是完全递增的，但分析起来更复杂。 这些方法的效率和收敛速度在几个方面也有所不同。

​	剩下的两章描述了如何将这三类方法结合起来，以获得它们每一个的最佳特征。 在其中一章中，我们描述了如何通过使用资格痕迹将蒙特卡洛方法的优势与时间差分方法的优势相结合。 在本书最后一章中，我们将展示如何将时间差异学习方法与模型学习和规划方法（如动态规划）结合起来，为表格强化学习问题提供完整统一的解决方案。



## Chapter 2 Multi-armed Bandits（武装海盗问题）

​	将强化学习与其他类型学习区分开来的最重要特征是它使用训练信息来评估所采取的行动，而不是通过正确的行动来指导。 这就是创造积极探索的需求，以便明确地寻找良好的行为。纯粹的评估反馈表明采取的行动有多好，但不是它是可能的最好还是最差的行动。 另一方面，纯粹的指导性反馈表明要采取正确的行动，与实际采取的行动无关。 这种反馈是监督学习的基础，其中包括大部分模式分类，人工神经网络和系统识别。 在纯粹的形式中，这两种反馈非常明显：评估反馈完全取决于所采取的行动，而指导性反馈与所采取的行动无关。

​	在本章中，我们将用简化的方式进行强化学习的评估方面的研究和学习，本章不涉及学习在多种情况下的行为。 这种非关联性设置是大多数涉及评估反馈的先前工作已完成的设置，它避免了完全强化学习问题的大部分复杂性。 研究这个案例使我们能更加清楚的看评估反馈如何与指导性反馈不同之处，并且可以与指导性反馈相结合。

​	我们探索的特定非关联性，评估性反馈问题是一个简单版本的k臂式强盗问题。 我们用这个问题来介绍一些基本的学习方法，我们在后面的章节中将其扩展到全面的强化学习问题。 在本章的最后，我们通过讨论当土匪问题变为关联性问题时会发生什么，即在多种情况下采取行动时，我们更接近完全强化学习问题。

###2.1 A k-armed Bandit Problem（K 臂海盗问题）

​	考虑以下学习问题。 您不断面临k个不同选项或操作中的选择。 在每次选择后，您会收到一个数值奖励，该数值奖励取决于您选择的行动的平稳概率分布。 您的目标是最大化某段时间内的预期总回报，例如，超过1000个动作选择或时间步骤。

​	这是 k 臂强盗问题的原始形式，与老虎机或“独臂强盗”类似，除了它有 k 个杠杆而不是一个。 每个动作选择就像老虎机的一个杠杆，而奖励就是赢得大奖的收益。 通过重复的行动选择，您可以通过将您的行为集中在最好的杠杆上来最大化您的奖金。 另一个比喻是医生在一系列重病患者的实验性治疗中进行选择。 每个行动都是选择一种治疗，每个奖励都是患者的生存或幸福。 今天，术语“强盗问题”有时用于上述问题的概括，但在本书中我们用它来指代这个简单的例子。

​	在我们的k-手臂匪徒问题中，考虑到该行动被选中，k个行动中的每一个行动都具有预期的或平均的奖励; 现在让我们将其称之为该行动的价值。 我们表示在时间步骤 $t$ 选择的行动为 $A_t$ ，相应的回报为 $R_t$ 。 表示为$q _*(a)$ 的任意行为 $a$ 的值是给定 $a$ 被选择时的期望回报：
$$
q ∗ (a) = E[R t | A t = a] .
$$
如果你知道每个动作的价值，那么解决k臂式盗匪问题将是微不足道的：你肯定总是选择具有最高价值的动作。 我们假设你不知道确定行动值，尽管你可能对于行动值有部分自己的猜测。 我们将时间步骤 $t$ 中的动作 $a$ 的估计值表示为 $Q_t(a)$ 。 我们希望 $Q_t(a)$ 接近 $q_*(a)$ 。	

​	如果您保持着行动值的估计值，那么在任何时间步中至少有一个行动的估计值最大。我们称之为贪婪行为。当您选择其中一项行动时，我们会说您正在**利用**您当前对行动价值的了解。如果您选择其中一项非强制性操作，那么我们说您正在**探索**，因为这样可以提高您对非强制性操作价值的估计。探索是在一步之内最大限度地实现预期回报的正确方法，但从长远来看，探索可能会产生更大的总回报。例如，假设贪婪行为的价值是确定的，而其他几种行为估计几乎一样好，但具有很大的不确定性。不确定性是这样的，至少其中一个其他行为可能实际上比贪婪行为更好，但你不知道哪一个行为。如果你有很多时间步骤来做出行动选择，那么探索非实际行动并发现哪些行为比贪婪行为更好可能会更好。**在短期内，在探索期间奖励较低，但从长远来看，奖励较高**，因为在发现更好的行动后，您可以多次利用它们。因为不可能通过任何单一的行动选择来探索和利用，所以经常提到探索与利用之间的“冲突”。

​	在任何特定情况下，探索或利用两种操作哪个更好是一个复杂的问题，相对而言取决于估计值，不确定性和剩余步骤的数量。 有许多复杂的方法可用于平衡探索和利用特定数量的K型武装强盗及相关问题。 但是，这些方法中的大多数都会对此作出有力的假设平稳性和先验知识，而这些知识在应用程序中被违反或无法验证，以及我们在后续章节中考虑的全面强化学习问题。 当这些方法的假设不适用时，这些方法的最优性或有界损失的保证是很不安全的。

​	在本书中，我们并不担心平衡探索和利用的方法过于复杂。 我们只担心怎么样能平衡他们。 在本章中，我们针对 K 臂海盗问题问题提出了几种简单的平衡方法，并表明它们比始终利用的方法要好得多。 平衡勘探和开采的需求是一个强化中出现的独特挑战学习; 我们版本的K臂土匪问题的简单性使我们能够以一种特别明确的形式展示这一点。