---
layout:     post
title:      强化学习导论（八）
subtitle:   
date:       2018-5-10
author:     度朝腾
header-img: img/post-bg-universe.jpg
catalog: true
tags:
    - python
---

##2.2 行动价值法（Action-value Methods）

​	我们首先仔细研究一些简单的方法来估计行动的价值和用估计值做出行动选择决策。 回想一下，行动的真正价值是选择行动时的平均回报。 估计这种情况的一种自然方法是平均实际收到的回报:
$$
Q t (a) \doteq \begin {array}{ccc} sum \quad of \quad rewards \quad when \quad a \quad taken \quad prior \quad to \quad t\\ \hline number \quad of \quad times \quad a \quad taken \quad prior \quad to \quad t\end {array} = \begin {array}{ccc} \sum_{i=1}^{t-1}R_i \cdot I_{A_i=a} \\ \hline \sum_{i=1}^{t-1}I_{A_i=a}\end {array}
$$

其中 1 个谓词表示随机变量，如果谓词为真则为 1 ，否则为 0 。 如果分母为零，那么我们将 $Q_t(a)$ 定义为一些默认值，例如 0 .当分母趋于无穷大时，根据大数定律， $Q_t(a)$ 收敛于 $q_*(a)$ 。 我们称之为估计行动值的样本平均法，因为每个估计值都是相关奖励样本的平均值。 当然，这只是评估行动价值的一种方式，并不一定是最好的一种。但是，现在让我们继续使用这种简单的评估方法，然后讨论如何使用估计值来选择行动。

​	最简单的动作选择规则是选择具有最高估计值的动作之一，即前一节中定义的贪婪动作之一。 如果存在多个贪婪行为，那么以某种任意方式（可能是随机的）进行选择。 我们写这个贪婪的动作选择方法为:
$$
A_t = argmax_a Q_t(a)
$$
其中 $argmax_a$ 表示动作 $a$ ，后面的表达式被最大化（再次，任意断开关系）。贪婪的行为选择总是利用当前的知识来最大化立即奖励;它不会花费时间抽取显然较差的行动来看看他们是否真的会更好。一个简单的选择是在大多数时间贪婪地行为，但每隔一段时间，以小概率 $ε$ 说，而不是从动作值估计中，以相同的概率从所有动作中随机选择。我们使用这个接近贪婪的动作选择规则 $ε-greedy$ 方法来调用方法。这些方法的一个优点是，在步数增加的极限内，每个动作都将被无限次地采样，从而确保所有 $Q_t(a)$ 收敛于 $q_*(a)$ 。当然，这当然意味着选择最优行为的概率收敛于大于 $1 - ε$ ，即接近确定性。然而，这些只是渐近的保证，并且对这些方法的实际有效性没有多少意见。

### 算法一 : 

​	在 $ε-greedy$ 的行为选择中，对于有两个行为并且 $ε= 0.5$ 的情况，贪婪行为被选中的概率是什么？

###算法二：

​	强盗示例中，考虑带 $k = 4$ 行为的 $k$ 形武装强盗问题，记为考虑应用这个问题的一个算法，使用 $ε-greedy$ 行为选择，样本平均行动值估计，以及初始估计 $Q_1(a)= 0$ ，对于所有 $a$ 来说。 假设动作和奖励的初始顺序为 $A_1 = 1，R_1 = 1，A_2 = 2，R_2 = 1，A_3 = 2，R_3 = 2，A_4 = 2，R_4 = 2，A_5 = 3，R_5 = 0$。在这些时间步骤中的一些情况下，可能发生了 $ε$ 情况，导致随机选择动作。 这确实发生在哪些时间步骤？ 在哪个时间步骤可以这样做可能发生了？

##2.3 十臂测试 

​	为了粗略地评估 $greedy$ 和 $ε-greedy$ 方法的相对有效性，我们在一组测试问题上对它们进行了数值比较。这是一组2000年随机生成的k-臂海盗问题，k = 10。对于每个匪问题，例如图2.1所示的问题，动作值 $q_*(a) ，a = 1，...。 。 。 ，10$ ，根据平均为 $0$ 和方差为1的正态（高斯）分布来选择。然后，当应用于该问题的学习方法在时间步骤t选择动作 $A_t$ 时，实际回报 $R_t$ 从均值 $q_*(A_t)$ 和方差1的正态分布。这些分布在图2.1中以灰色显示。我们将这套测试任务称为10个测试平台。对于任何学习方法，我们都可以测量其性能和行为，因为随着经验超过1000个时间步长，应用于其中一个强盗问题时，性能和行为会得到改善。这构成了一次运行。重复2000次独立运行，每次都有不同的海盗问题，我们获得了学习算法的平均行为的度量。

​	ε贪婪比贪婪方法的优势取决于任务。例如，假设奖励方差已经较大，例如10而不是1.随着奖励越大，需要更多的探索才能找到最佳的行动，相对于贪婪方法，ε-贪婪方法应该更好。另一方面，如果奖励差异为零，则贪婪方法在尝试一次之后就会知道每个动作的真实值。在这种情况下，贪婪的方法可能实际上表现最好
因为它很快会找到最佳的行动，然后永远不会探索。但即使在确定性情况下，如果我们削弱其他一些假设，也有很大的优势。例如，假设土匪任务是非稳定的，也就是说，行动的真实价值随时间而改变。在这种情况下，即使在确定性的情况下也需要进行探索，以确保其中一个非实质行为没有变得比贪婪行为更好。我们将在接下来的几页中看到章节中，非平稳性是强化学习中最常遇到的情况。即使潜在的任务是平稳的和确定性的，学习者也会面临一系列强盗决策任务，随着学习的进行和代理人政策的变化，每个任务都随时间而变化。强化学习需要在勘探和开发之间取得平衡。	

## 2.4 增量实现功能

我们迄今为止所讨论的行动价值方法都将行动价值评估为观察到的奖励的样本平均值。 我们现在转向如何以有效的计算方式计算这些平均值的问题，特别是具有恒定记忆和恒定时间步长的计算问题。

​	为了简化表示方法，我们将注意力集中在一个动作上。让我现在表示在第 $i$ 次选择这个动作后收到的奖励，并让 $Q_n$ 表示在它被选择 $n - 1$ 次后它的动作值的估计值，我们现在可以简写为：
$$
Q_n \doteq \begin {array}{ccc} R_1 + R_2 + \cdot\cdot\cdot + R_{n-1}\\ \hline  n-1\end {array}
$$
显而易见的实现方式是维持所有奖励的记录，然后在需要估计值时执行此计算。 但是，如果这样做了，那么随着时间的推移，内存和计算需求会随着更多的回报而增长。 每个额外的奖励都需要额外的存储空间来存储它和额外的计算来计算分子的总和。

​	正如你可能会怀疑的那样，这并不是真的有必要。 用更小的，不变的计算来处理每个新的奖励，可以很容易地设计出用于更新平均值的增量公式。 给定 $Q_n$ 和第 $n$ 个奖励 $R_n$ ，所有 $n$ 个奖励的新平均值可以由下式计算:
$$
Q_{n+1} = Q_n + 1/n[R_n - Q_n]
$$
即使对于 $n = 1$ 也保持不变，对于任意 $Q_1$ 获得 $Q_2 = R_1$ 。 这个实现需要的内存只有 $Q_n$ 和 $n$ ，并且每个新的奖励只需要小的计算（2.3）。 在下一页的框中显示使用递增计算的样本平均值和 $ε-greedy$ 行为选择的完整海盗算法的伪代码。 假设函数 $bandit(a)$ 采取行动并返回相应的奖励。