---
layout:     post
title:      强化学习导论（四）
subtitle:   
date:       2018-4-16
author:     度朝腾
header-img: img/post-bg-universe.jpg
catalog: true
tags:
    - python
---

## 强化学习导论

### 局限性与适用范围

> 转载自：https://blog.csdn.net/thousandsofwind/article/details/79725198

从前面的讨论中，应该清楚的是，强化学习很大程度上依赖于状态的概念。他既作为对策略和值函数的输入，也作为模型的输入和输出。非正式地，我们可以把状态看作是传达给代理的某种特定时期“环境如何”的信号。状态的形式定义在第3章中给出的马尔可夫决策过程的框架中给出的。然而，更普遍的是，我们鼓励读者遵循非正式的含义，并将状态视为代理对其环境所能获得的任何信息。实际上，我们假设状态信号是由某些预处理系统产生的，而预处理系统是代理环境的一部分。在本书中，我们没有讨论构造、改变或学习状态信号的问题。我们采取这种做法不是因为我们认为状态不重要，而是为了充分关注决策问题。换言之，我们主要关心的不是设计状态信号，而是设计行为函数以应对各种状态。（我们在第17.3章的最后节章中简单介绍了状态的设计和建设）。

​    我们在本书中所考虑的大部分强化学习方法都是围绕估值函数来构造的，但它对于解决强化学习问题而言并不是必须的。例如，遗传算法、遗传规划、模拟退火和其他优化方法（genetic algorithms, genetic programming, simulated annealing, and other optimization methods）已被用于研究强化学习问题，而不必求助于值函数。这些方法评估了许多非学习代理的“终身”行为，每个使用不同的策略与环境交互，选择那些能够获得最多奖励的人。我们称其为进化方法，是因为它们类似于生物进化，生物进化的产物能够熟练行为，即使它们在个体寿命期间没有学习。如果策略的空间足够小，或者策略容易被构造，或者如果有大量的时间可用于搜索，则进化方法可以是可取的。此外，进化方法在学习代理不能感知环境的完整状态的问题上具有优势。

​    我们的重点是强化学习方法，学习与环境相互作用，而进化方法不这样做。在许多情况下，能够利用个体行为交互细节的方法比进化方法更有效。发展固定方法忽略了对大量的强化学习问题的有用的结构：他们没有利用政策从状态到行动的映射这一事实；他们并没有注意到个体生命周期中所经历的状态和采取的行动。在某些情况下，这种信息可能是误导性的（例如，当状态不确定的时候），但更经常的是它能使搜索更有效率。虽然进化和学习有许多共同的特性并且自然地协同工作，但我们不认为进化方法本身特别适合强化学习问题，因此，我们不在本书中讨论它们。

​    然而，我们确实包含了一些方法，像进化方法一样不求助于值函数的方法。这些方法在由数值参数集合定义的策略空间中进行搜索。他们估计参数应该调整的方向，以迅速地改善策略的表现。然而，与进化方法不同，它们在代理与环境交互时产生这些估计，因此可以利用个体行为交互的细节。像这样的方法在许多问题中被证明是有用的，一些最简单的强化学习方法属于这一类（见第13章）。然而，这种类型的最佳方法往往以某种形式包含值函数。