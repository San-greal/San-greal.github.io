## 强化学习导论

### 1.5 拓展例子：井字棋

> https://blog.csdn.net/thousandsofwind/article/details/79745086

(注：尝试了很多次就是没有办法发全文，迷)

为了说明强化学习的一般概念，并与其他方法进行对比，我们接下来将更详细地考虑一个示例。   

​     想想我们熟悉的孩子玩的井字棋。棋手在三行三列的棋盘上博弈，一个棋手下X，另一个下O。若X或O的连续三个棋子落于一行或一列或同一斜线上则获胜；若棋盘被填满也不能决出胜负则为平局。让我们假设，我们是在和一个不完美的棋手比赛，他的战术有时是不正确的，并且允许我们获胜。而且，让我们考虑平局和亏损对我们同样不利。我们如何构建一个能在对手的游戏中发现玩家的漏洞，并取得最大化获胜的机会的机器棋手？

​    虽然这是一个简单的问题，但传统方法难以很好地解决。例如，博弈论中经典的“极小极大”解在这里是不正确的，因为对手可能有特殊的博弈方式。极小极大棋手从不去他可能输棋的局面，就是由于对手的失误他可能能胜出（For example, a minimax player would never reach a game state from which it could lose, even if in fact it always won from that state because of incorrect play by the opponent）。经典的连续决策问题的最优方法，如动态规划，可以计算出任何对手的最优解，但要求输入该对手的全部规格，包括对手在每个基础状态（board state，不明白）下移动的概率。让我们假设在问题无法获得这些先验信息，因为它不适用于大多数实际问题。另一方面，这样的信息可以从经验中估计，在这个例子上就是和对手下棋。最好能学习对手的行为模式，达到一定的置信水平（some level of confidence），然后应用动态规划来计算近似的对手模型的最优解。这与我们在本书后面研究的强化学习方法没有什么不同。

​    这个问题的一种进化方法将直接搜索一个策略空间以找到最可能赢的那个。在这里，一个策略是告诉玩家在游戏中的各个状态下如何进行移动的规则--既3*3的板子上x和o所有可能摆放的位置。我们考虑策略中，获胜的估计概率将通过与对手进行一些比赛来获得。这一评估将决定下一步将考虑哪个或哪些策略。一种典型的演化方法是在策略空间中进行爬坡，然后在尝试进步的过程中依次生成和评估策略。或者，也许可以使用遗传算法来维护和评估一组策略。实际上，我们有上百种不同的优化方法。

​    下面是使用值函数的方法来解决黑白棋问题的方法。首先，我们建立了一个数字表，每个数字对应一个游戏状态。每一个数字将是我们获胜的最新估计。我们把这个估计看作是状态的价值，整个表是学习的值函数。如果A的胜率高于比，我们就认为A的价值高于B，或A比B更好。假设我们玩X，那么所有三个X居于一列的获胜的概率是1，因为我们已经赢了。同样地，对于所有三个O居于一列，或者所有三列被“填充”（or that are “filled up），获胜的概率是0，因为从那时起我们就不可能赢了。我们将所有其他状态的初始值设为0.5，表示猜测我们有50%的获胜机会。

​    我们和对手打了许多场比赛。为了选择我们的动作，我们检查每个动作可能产生的状态（在棋盘上的每个空格中有一个），并在表中查找它们当前的值。大多数时候，我们贪婪地移动，选择最有价值的运动，也就是说，以最高的获胜概率。然而，偶尔我们会从其他动作中随机选择。这些被称为探索性的行动，因为它们使我们体验到我们可能从未见过的状态。在游戏中移动和考虑的序列可绘制成图1.1。

图1.1：一系列井字棋移动。实线表示游戏中所采取的动作；虚线表示我们（强化学习播放器）考虑但未做的动作。我们的第二次移动是一个探索性的举措，这意味着e*所表示的移动在等级上优于当前举动。探索性的动作不会导致任何学习，但我们的其他动作都会产生更新，导致了如弧线箭头所显示的那种更新--如文中所写，估值函数自下而上的产生了变化。

​    当我们下棋的时候，我们按游戏中的发现不断改变状态的值，使这些值能更准确地估计获胜的可能性。为了做到这一点，在贪婪移动后，我们重写前一状态的值（we “back up” the value of the state after each greedy move to the state before the move）。更准确地说，前一状态的当前值被更新以接近后续状态的值。这可以通过将前一状态值稍向后状态值靠近一小部分来完成（This can be done by moving the earlier state’s value a fraction of the way toward the value of the later state）。如果让s表示在移动前的状态，s‘表示移动后状态，那么将s的估计值的更新表示为V（s），可以写成

V(s) ← V(s) +